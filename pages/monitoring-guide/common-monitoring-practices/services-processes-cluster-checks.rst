.. _mg-services-processes-cluster-checks:

Services, Processes and Clusters Checks
+++++++++++++++++++++++++++++++++++++++

Service checks
  Checking the availability of the OpenStack services from the point
  of view of the user is absolutely necessary in order to make sure
  that all the services respond to the user requests as expected.

  Those service checks should be performed on a regular basis using
  synthetic transactions that perform various HTTP requests against
  the OpenStack service endpoints. The service checks should return
  an availability status that is either ``pass`` or ``failed``.

  The synthetic transactions should use a dedicated *project*
  (*tenant*) and *user* so that it is always possible to differentiate
  between the load generated by the service checks versus the load
  generated by the genuine activity.

  The service checks should be performed against the service endpoints
  using the HAProxyâ€™s **Virtual IP** (VIP) to ensure that both the
  service APIs and as the HAProxy, which distributes the load above
  them, are traversed by the HTTP requests.

  The service checks should strive to minimize the **observer effect**
  by using lightweight and non-intrusive (read only) requests. The
  service checks should also avoid using requests that propagate to
  other services than the service being watched. A general rule of
  thumb to avoid overloading the system is to use a non-aggressive
  polling interval as a tradeoff between being alerted of errors
  quickly and generating a load that is too heavy.

  The service checks should also strive to record the response time of
  synthetic transactions in order to establish a performance baseline
  and produce statistics (average, mean, percentile, standard
  deviation) that can be used to detect anomalies.

Process checks
  Process checks can be performed either remotely (via SSH) or
  locally using a monitoring agent running on each of the OpenStack
  nodes. The goal of process checks is to ensure that all the processes
  participating in the support of an OpenStack role, such as controller,
  are indeed up and running. Those processes include the service
  endpoints (nova-api), the service workers connected to the AMQP bus
  (nova-scheduler), as well as the various processes supporting
  the auxiliary functions such as RabbitMQ, MySQL and so forth.

  The process checks should also ensure that the OpenStack processes
  are bound to their respective **networking ports**.

Cluster checks
  The :ref:`Mirantis OpenStack Planning Guide <planning-guide>`
  mandates the deployment of an HA cluster supporting different
  availability functions in the cloud infrastructure. For example,
  the service endpoints are distributed in an active/active HA cluster
  along with an HAProxy running on top of them on each controller
  node. There is only one active HAProxy at a time that is responsible
  for detecting service endpoints failures and distributing the load
  between them. The high availability of the HAProxy itself is
  supported by an active/passive HA cluster based on Corosync and
  Pacemaker. Corosync and Pacemaker are collectively responsible for
  ensuring the transparent failover of the HAProxy and VIP when the
  master node is failed. Same thing for the MySQL database. So a
  correct appraisal of the OpenStack services' availability status
  depends on the HA cluster healthiness appraisal. That is why we
  stated earlier that the criticality of an OpenStack error should be
  evaluated differently depending on whether the HA functions of the
  cloud infrastructure are still being ensured by the HA cluster or
  not. In a nutshell, the HA functions are not being ensured anymore
  when there is no more failover node available in the cluster or
  when the HA cluster itself is broken.
